# Testing document

## Overview of the tests

Application has been tested both manually and automatically with unit and integration tests with unittest. Manual testing for the relevance of results has been started.

## Unit and integration tests

Unit and integration tests are automated and cover all classes, excluding UI, following common approach in the faculty's classes.

Unit and integration tests can be run with the following command: 

```bash
poetry run invoke test
```

Unit and integration tests are also automatically run as part of the application's GitHub Actions CI pipeline when the the application is pushed to the GitHub repository. Results are visible in codecov. 

![GitHub Actions](https://github.com/katriryt/Tiralabra-2022-VerbumReprehendo/workflows/CI/badge.svg)

[![codecov](https://codecov.io/gh/katriryt/Tiralabra-2022-VerbumReprehendo/branch/main/graph/badge.svg?token=2QWJAKX877)](https://codecov.io/gh/katriryt/Tiralabra-2022-VerbumReprehendo)

### Services (core functionality)

`SpellCheck` and `DistanceHeuristics` classes are tested with [TestSpellCheck](../src/tests/spell_check_test.py) and [TestDistanceHeuristics](../src/tests/heuristics_test.py) test classes, respectively. Inputs used are strings of words with English and Scandinavian alphabets, to check that input checks are working correctly.

### Repositories Classes

Repository classes `TrieNode` and `Trie` are tested with [TestTrie](../src/repositories/trie.py) test class. `English Dictionary` is covered through other test classes' tests. 

### Test coverage

An HTML-format test coverage report can be generated with the following command:

```bash
poetry run invoke coverage-report
```

The report is generated in the _htmlcov_ folder.

Please see the overall test coverage below. 

![](./pictures/coverage_report.png)

Tests focus on the most relevant methods and features.

## System testing

Application's test in different operating system - Windows and Ubuntu - has been done manually. Automated tests are run on both environments.

## Other testing

Tests for the relevance of suggestions have been done manually. In the tests, suggestions for the misspelled words, generated by the different distance metrics (simplistic approach, Levenshtein, Optimal string alignment, and Damerau-Levenshtein distances) with/without keyboard heuristics, and considering the frequency of the word used in the English language, are compared. 

Manual tests suggest that the algorithms are at the moment quite capable of suggesting the original word intended within the top 10 best alternative words. There does not seem to be difference in the performance of the different distance metrics. Using keyboard heuristics seems to improve the suggestions, and using frequencies improves the relevance of suggestions quite significantly. However, it is important to prioritize suggestions first based on keyboard heuristic, and only after that use the frequencies. If frequencies are used in the priorization with a larger weight than the distance metrics, the results become highly inaccurate, i.e., the algorithm suggests completely wrong words.

Automatic tests to verify this with a larger data set are to be conducted. In the relevance tests, [a list of most typically misspelled English words](https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines) will taken (available in e.g. Wikipedia), and they will be run through the applications algorithm to see how the current algorithm fares in identifying the intended word.

Based on instructor's feedback, tests for application's performance (e.g. how quickly the different metrics can suggest spellchecked words) are not planned. However, during the manual relevance testing, it was noted that when the misspelled words are long, the application seems a bit slow, so there might be need / opportunities to speed up the search for alternative words.

## Remaining quality errors

Pylint and autopep8 are used to help identifying and correcting for quality errors.
